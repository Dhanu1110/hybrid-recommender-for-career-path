{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Recommender System - Quick Demo\n",
    "\n",
    "This notebook demonstrates the complete career recommendation pipeline using synthetic data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The system consists of three stages:\n",
    "1. **Path Generation**: BERT4Rec model generates candidate career paths\n",
    "2. **Skill Gap Analysis**: Analyzes feasibility based on user skills and ESCO knowledge\n",
    "3. **Resource Recommendation**: Suggests learning resources for skill gaps\n",
    "\n",
    "Let's walk through each component with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# Import our modules\n",
    "from ingest.download_and_prepare import create_sample_data, DataIngestionPipeline\n",
    "from ingest.esco_loader import create_esco_loader\n",
    "from ingest.text_to_esco_mapper import create_text_mapper\n",
    "from models.bert4rec import BERT4RecConfig, create_bert4rec_model\n",
    "from models.train_path_model import create_synthetic_data, CareerSequenceDataset\n",
    "from reasoner.skill_gap import create_skill_gap_analyzer\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Synthetic Data\n",
    "\n",
    "First, let's create some synthetic data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample ESCO data\n",
    "print(\"Creating sample ESCO data...\")\n",
    "create_sample_data()\n",
    "\n",
    "# Run data ingestion pipeline\n",
    "print(\"Running data ingestion pipeline...\")\n",
    "pipeline = DataIngestionPipeline()\n",
    "success = pipeline.run()\n",
    "\n",
    "if success:\n",
    "    print(\"‚úÖ Data ingestion completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Data ingestion failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the processed data\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "\n",
    "print(\"Processed data files:\")\n",
    "for file in processed_dir.glob(\"*.parquet\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    print(f\"  {file.name}: {len(df)} records\")\n",
    "    print(f\"    Columns: {list(df.columns)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize ESCO Knowledge Graph\n",
    "\n",
    "Load the ESCO knowledge graph for skill analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ESCO loader\n",
    "print(\"Initializing ESCO knowledge graph...\")\n",
    "esco_loader = create_esco_loader(\"../data/processed\")\n",
    "\n",
    "# Test ESCO functions\n",
    "print(\"\\nTesting ESCO functions:\")\n",
    "\n",
    "# Test get_job_skills\n",
    "job_skills = esco_loader.get_job_skills('occ_001')\n",
    "print(f\"Skills for job 'occ_001': {job_skills}\")\n",
    "\n",
    "# Test get_skill_parents\n",
    "skill_parents = esco_loader.get_skill_parents('skill_001')\n",
    "print(f\"Parents for skill 'skill_001': {skill_parents}\")\n",
    "\n",
    "# Test get_skill_distance\n",
    "distance = esco_loader.get_skill_distance('skill_001', 'skill_002')\n",
    "print(f\"Distance between skill_001 and skill_002: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text-to-ESCO Mapping\n",
    "\n",
    "Test the text mapping functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text mapper\n",
    "print(\"Initializing text-to-ESCO mapper...\")\n",
    "try:\n",
    "    text_mapper = create_text_mapper(\"../data/processed\")\n",
    "    \n",
    "    # Test occupation mapping\n",
    "    print(\"\\nTesting occupation mapping:\")\n",
    "    job_matches = text_mapper.map_text_to_occupations(\"software developer\", top_k=3)\n",
    "    for match in job_matches:\n",
    "        print(f\"  {match['title']} (ID: {match['esco_id']}, Score: {match['score']:.3f})\")\n",
    "    \n",
    "    # Test skill mapping\n",
    "    print(\"\\nTesting skill mapping:\")\n",
    "    skill_matches = text_mapper.map_text_to_skills(\"programming\", top_k=3)\n",
    "    for match in skill_matches:\n",
    "        print(f\"  {match['title']} (ID: {match['esco_id']}, Score: {match['score']:.3f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Text mapper initialization failed (likely missing dependencies): {e}\")\n",
    "    print(\"Continuing with manual mapping...\")\n",
    "    text_mapper = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: BERT4Rec Model Training (Synthetic Data)\n",
    "\n",
    "Create and train a small BERT4Rec model on synthetic career sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic career sequences\n",
    "print(\"Creating synthetic career sequences...\")\n",
    "sequences, job_to_id = create_synthetic_data(num_sequences=100, vocab_size=20)\n",
    "\n",
    "print(f\"Created {len(sequences)} sequences\")\n",
    "print(f\"Vocabulary size: {len(job_to_id)}\")\n",
    "print(f\"Sample sequence: {sequences[0]}\")\n",
    "print(f\"Job vocabulary: {list(job_to_id.keys())[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small BERT4Rec model for demo\n",
    "print(\"Creating BERT4Rec model...\")\n",
    "config = BERT4RecConfig(\n",
    "    vocab_size=len(job_to_id),\n",
    "    d_model=64,  # Small for demo\n",
    "    n_layers=2,  # Small for demo\n",
    "    n_heads=4,\n",
    "    max_seq_len=20\n",
    ")\n",
    "\n",
    "model = create_bert4rec_model(config)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model inference (without training for demo)\n",
    "print(\"Testing model inference...\")\n",
    "\n",
    "# Create a sample input sequence\n",
    "sample_sequence = torch.tensor([sequences[0][:5]], dtype=torch.long)  # First 5 jobs\n",
    "print(f\"Input sequence: {sample_sequence}\")\n",
    "\n",
    "# Generate next job recommendations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recommendations = model.generate_next_jobs(sample_sequence, top_k=5)\n",
    "    \n",
    "print(\"\\nTop 5 next job recommendations:\")\n",
    "for i, (job_id, prob) in enumerate(recommendations[0]):\n",
    "    job_name = [k for k, v in job_to_id.items() if v == job_id][0] if job_id in job_to_id.values() else f\"job_{job_id}\"\n",
    "    print(f\"  {i+1}. {job_name} (ID: {job_id}, Prob: {prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Skill Gap Analysis\n",
    "\n",
    "Test the skill gap reasoning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize skill gap analyzer\n",
    "print(\"Initializing skill gap analyzer...\")\n",
    "try:\n",
    "    skill_analyzer = create_skill_gap_analyzer(\"../configs/system_config.yaml\")\n",
    "    \n",
    "    # Define a sample user with some skills\n",
    "    user_skills = {'skill_001', 'skill_002'}  # User has Python and Data Analysis\n",
    "    print(f\"User skills: {user_skills}\")\n",
    "    \n",
    "    # Define candidate career paths with mock probabilities\n",
    "    candidate_paths = [\n",
    "        (['occ_001', 'occ_002'], 0.8),  # Software Engineer -> Data Scientist\n",
    "        (['occ_001', 'occ_003'], 0.6),  # Software Engineer -> Marketing Manager\n",
    "        (['occ_002', 'occ_003'], 0.4),  # Data Scientist -> Marketing Manager\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nCandidate paths: {candidate_paths}\")\n",
    "    \n",
    "    # Analyze all paths\n",
    "    analyses = skill_analyzer.analyze_multiple_paths(user_skills, candidate_paths)\n",
    "    \n",
    "    print(\"\\nPath Analysis Results:\")\n",
    "    for i, analysis in enumerate(analyses):\n",
    "        print(f\"\\nPath {i+1}: {' -> '.join(analysis.path)}\")\n",
    "        print(f\"  Model Probability: {analysis.model_prob:.3f}\")\n",
    "        print(f\"  Feasibility Score: {analysis.feasibility_score:.3f}\")\n",
    "        print(f\"  Combined Score: {analysis.combined_score:.3f}\")\n",
    "        print(f\"  Total Missing Skills: {analysis.total_missing_skills}\")\n",
    "        \n",
    "        # Show per-job gaps\n",
    "        for job_id, gap in analysis.per_job_gaps.items():\n",
    "            print(f\"    {job_id}: {len(gap.missing_skills)} missing skills, gap score: {gap.gap_score:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Skill gap analyzer failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Learning Plan\n",
    "\n",
    "Create a learning plan for the best career path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning plan for the best path\n",
    "if 'analyses' in locals() and analyses:\n",
    "    best_path = analyses[0]  # Highest combined score\n",
    "    \n",
    "    print(f\"Learning Plan for Best Path: {' -> '.join(best_path.path)}\")\n",
    "    print(f\"Combined Score: {best_path.combined_score:.3f}\")\n",
    "    \n",
    "    learning_plan = skill_analyzer.get_learning_plan(best_path)\n",
    "    \n",
    "    if learning_plan:\n",
    "        print(\"\\nRecommended Learning Sequence:\")\n",
    "        for job_id, skills_to_learn in learning_plan.items():\n",
    "            print(f\"\\nFor {job_id}:\")\n",
    "            for i, skill_id in enumerate(skills_to_learn, 1):\n",
    "                print(f\"  {i}. {skill_id}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No additional skills needed - you're ready for this path!\")\n",
    "    \n",
    "    # Generate explanation\n",
    "    explanation = skill_analyzer.explain_path_feasibility(best_path)\n",
    "    print(\"\\nPath Feasibility Explanation:\")\n",
    "    print(f\"  Overall Score: {explanation['overall_score']:.3f}\")\n",
    "    print(f\"  Model Confidence: {explanation['model_confidence']:.3f}\")\n",
    "    print(f\"  Feasibility: {explanation['feasibility']:.3f}\")\n",
    "    print(f\"  Total Missing Skills: {explanation['total_missing_skills']}\")\n",
    "    \n",
    "    if explanation['recommendations']:\n",
    "        print(\"\\nRecommendations:\")\n",
    "        for rec in explanation['recommendations']:\n",
    "            print(f\"  ‚Ä¢ {rec}\")\n",
    "else:\n",
    "    print(\"No path analyses available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Summary\n",
    "\n",
    "Let's summarize what we've demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Career Recommender System Demo Complete!\")\n",
    "print(\"\\nWhat we demonstrated:\")\n",
    "print(\"‚úÖ Data ingestion pipeline with synthetic ESCO data\")\n",
    "print(\"‚úÖ ESCO knowledge graph loading and querying\")\n",
    "print(\"‚úÖ Text-to-ESCO mapping (if dependencies available)\")\n",
    "print(\"‚úÖ BERT4Rec model creation and inference\")\n",
    "print(\"‚úÖ Skill gap analysis and feasibility scoring\")\n",
    "print(\"‚úÖ Learning plan generation\")\n",
    "print(\"‚úÖ Path feasibility explanation\")\n",
    "\n",
    "print(\"\\nNext steps to run with real data:\")\n",
    "print(\"1. Replace synthetic data with real Karrierewege and ESCO datasets\")\n",
    "print(\"2. Train BERT4Rec model on real career sequences\")\n",
    "print(\"3. Build resource recommendation system (Stage 3)\")\n",
    "print(\"4. Create Streamlit web application\")\n",
    "print(\"5. Set up FastAPI backend\")\n",
    "print(\"6. Implement evaluation framework\")\n",
    "\n",
    "print(\"\\nüìÅ Check the following directories for generated files:\")\n",
    "print(\"  - data/raw/ - Sample input data\")\n",
    "print(\"  - data/processed/ - Processed Parquet files\")\n",
    "print(\"  - configs/ - Configuration files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
